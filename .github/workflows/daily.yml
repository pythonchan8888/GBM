name: daily-predict

on:
  schedule:
    - cron: "0 6 * * *"  # daily at 06:00 UTC
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  build-and-publish:
    runs-on: ubuntu-latest
    env:
      DATABASE_URL: ${{ secrets.DATABASE_URL }}
      GBM_QUICK: ${{ vars.GBM_QUICK || '0' }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        shell: bash
        run: |
          if [ -f requirements.txt ]; then
            python -m pip install --upgrade pip
            pip install -r requirements.txt
          else
            python -m pip install --upgrade pip
            pip install numpy pandas requests lightgbm scikit-learn scipy seaborn matplotlib psycopg2-binary shap
          fi

      - name: Run daily prediction pipeline
        env:
            FOOTYSTATS_API_KEY: ${{ secrets.FOOTYSTATS_API_KEY }}
        run: |
          python gbm_dc_ev_model.py

      - name: Settle open bets (update P&L)
        if: ${{ env.DATABASE_URL != '' }}
        env:
          FOOTYSTATS_API_KEY: ${{ secrets.FOOTYSTATS_API_KEY }}
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          python - <<'PY'
          import os
          from gbm_dc_ev_model import settle_open_bets
          api_key = os.environ.get('FOOTYSTATS_API_KEY', '')
          db = os.environ.get('DATABASE_URL', '')
          settle_open_bets(api_key, db)
          PY

      - name: Prepare site folder
        run: |
          mkdir -p site
          # Copy ML model artifacts
          if [ -d artifacts/latest ]; then
            cp -r artifacts/latest/* site/ || true
          fi
          # Copy ParlayKing dashboard files (these will be the main site)
          cp index.html site/ || echo "Warning: index.html not found"
          cp styles.css site/ || echo "Warning: styles.css not found" 
          cp app.js site/ || echo "Warning: app.js not found"
          # Verify dashboard files were copied successfully
          if [ -f site/index.html ] && [ -f site/styles.css ] && [ -f site/app.js ]; then
            echo "✅ ParlayKing dashboard files copied successfully"
          else
            echo "❌ Warning: Some dashboard files missing"
            ls -la site/ || true
          fi

      - name: Export metrics and datasets to CSV (from Postgres)
        if: ${{ env.DATABASE_URL != '' }}
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        shell: bash
        run: |
          python - <<'PY'
          import os, csv
          import psycopg2
          from datetime import datetime
          
          db = os.environ.get('DATABASE_URL')
          if not db:
              raise SystemExit(0)
          
          try:
              conn = psycopg2.connect(db); cur = conn.cursor()
              # Latest run
              cur.execute("""
              select run_id, started_at, finished_at, train_rows, test_rows, poisson_home_loss, poisson_away_loss
              from runs order by finished_at desc nulls last limit 1
              """)
              latest = cur.fetchone()
              metrics_rows = []
              if latest:
                  keys = ["run_id","started_at","finished_at","train_rows","test_rows","poisson_home_loss","poisson_away_loss"]
                  for k, v in zip(keys, latest):
                      metrics_rows.append((k, v if v is not None else ""))
              # 30-day aggregates
              cur.execute("""
              select coalesce(sum(pl),0) as pnl_30d, coalesce(sum(stake),0) as stake_30d, count(*) as bets_30d
              from bets
              where dt_gmt8 > now() - interval '30 days'
              """)
              pnl_30d, stake_30d, bets_30d = cur.fetchone()
              roi_30d = (float(pnl_30d)/float(stake_30d)*100.0) if stake_30d and float(stake_30d)>0 else 0.0
              
              # Win rate and performance metrics (30-day)
              cur.execute("""
              select 
                count(case when pl > 0 then 1 end) as wins,
                count(case when pl >= 0 then 1 end) as non_losing,
                count(*) as total_bets,
                avg(case when pl > 0 then (pl/stake)*100.0 end) as avg_win_return_pct
              from bets 
              where dt_gmt8 > now() - interval '30 days' and stake > 0
              """)
              win_stats = cur.fetchone()
              if win_stats and win_stats[2] > 0:
                  win_rate = (float(win_stats[0]) / float(win_stats[2])) * 100.0
                  non_losing_rate = (float(win_stats[1]) / float(win_stats[2])) * 100.0
                  avg_win_return = float(win_stats[3] or 0)
              else:
                  win_rate = non_losing_rate = avg_win_return = 0.0
              
              metrics_rows += [
                  ("pnl_30d", pnl_30d), ("stake_30d", stake_30d), ("bets_30d", bets_30d), ("roi_30d_pct", roi_30d),
                  ("win_rate_30d_pct", win_rate), ("non_losing_rate_30d_pct", non_losing_rate), 
                  ("avg_win_return_30d_pct", avg_win_return)
              ]
              # Write metrics.csv
              os.makedirs('site', exist_ok=True)
              with open('site/metrics.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["metric","value"]); w.writerows(metrics_rows)
              # P&L by month & league
              cur.execute("""
              select to_char(date_trunc('month', dt_gmt8),'YYYY-MM') as month, league, sum(pl) as pnl
              from bets group by 1,2 order by 1,2
              """)
              rows = cur.fetchall()
              with open('site/pnl_by_month.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["month","league","pnl"]); w.writerows(rows)
              # Bankroll series (last 90 days)
              cur.execute("""
              select dt_gmt8, cum_bankroll from bets
              where dt_gmt8 > now() - interval '90 days' order by dt_gmt8
              """)
              rows = cur.fetchall()
              with open('site/bankroll_series_90d.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["dt_gmt8","cum_bankroll"]); w.writerows(rows)
              # Latest recommendations (7 days)
              cur.execute("""
              select dt_gmt8, league, home, away, rec_text, line, odds, ev, confidence
              from recommendations where dt_gmt8 >= now() - interval '7 days' order by dt_gmt8
              """)
              rows = cur.fetchall()
              with open('site/latest_recommendations.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["dt_gmt8","league","home","away","rec_text","line","odds","ev","confidence"]); w.writerows(rows)
              # Settled bets for parlay calculation (30 days)
              cur.execute("""
              select fixture_id, league, home, away, home_score, away_score,
                     line_betted_on_refined, bet_type_refined_ah, odds_betted_on_refined, 
                     stake, pl, status, dt_gmt8
              from bets where status = 'settled' and dt_gmt8 >= now() - interval '30 days' order by dt_gmt8 desc
              """)
              rows = cur.fetchall()
              with open('site/settled_bets.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["fixture_id","league","home","away","home_score","away_score","line_betted_on_refined","bet_type_refined_ah","odds_betted_on_refined","stake","pl","status","dt_gmt8"]); w.writerows(rows)
              cur.close(); conn.close()
              print("Exported metrics and datasets to site/.")
              
              # --- Build ROI heatmap (Tier x AH line) and Top Segments from bets ---
              import math
              import collections
              
              # Reconnect for a new cursor
              conn = psycopg2.connect(db); cur = conn.cursor()
              # Try to get bets with needed fields; tolerate missing columns
              cur.execute("""
              SELECT 
                COALESCE(league, '') as league,
                COALESCE(line_betted_on_refined, line, 0) as line,
                COALESCE(dynamic_league_tier, league_tier) as tier,
                COALESCE(stake, 0) as stake,
                COALESCE(pl, 0) as pl,
                dt_gmt8
              FROM bets
              WHERE dt_gmt8 > now() - interval '730 days'
              """)
              bets = cur.fetchall()
              cur.close(); conn.close()
          except Exception as e:
              print(f"Database connection failed: {e}")
              print("Creating fallback CSV files...")
              import math
              import collections
              os.makedirs('site', exist_ok=True)
              # Create empty fallback CSVs
              with open('site/metrics.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["metric","value"])
                  w.writerow(["status", "DB_unavailable"])
                  w.writerow(["generated_at", datetime.now().isoformat()])
              with open('site/pnl_by_month.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["month","league","pnl"])
              with open('site/bankroll_series_90d.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["dt_gmt8","cum_bankroll"])
              with open('site/latest_recommendations.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["dt_gmt8","league","home","away","rec_text","line","odds","ev","confidence"])
              with open('site/settled_bets.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["fixture_id","league","home","away","home_score","away_score","line_betted_on_refined","bet_type_refined_ah","odds_betted_on_refined","stake","pl","status","dt_gmt8"])
              bets = []  # Empty list for heatmap processing

          # Static fallback for tier mapping if DB rows don't contain a tier
          LEAGUE_TIERS = {
              'England Premier League': 1, 'Spain La Liga': 1, 'Germany Bundesliga': 1, 'Italy Serie A': 1, 'France Ligue 1': 1,
              'England Championship': 2, 'Netherlands Eredivisie': 2, 'Portugal Liga NOS': 2, 'Belgium Pro League': 2, 'Scotland Premiership': 2,
              'Turkey Süper Lig': 3, 'Austria Bundesliga': 3, 'Denmark Superliga': 3, 'USA MLS': 3, 'Brazil Serie A': 3,
              'Argentina Primera División': 3, 'Japan J1 League': 3, 'South Korea K League 1': 3, 'Saudi Arabia Professional League': 3
          }

          def normalize_line(x):
              # Bin to nearest 0.25 like -1, -0.75, ...
              try:
                  return round(float(x) * 4) / 4.0
              except Exception:
                  return 0.0

          heatmap = collections.defaultdict(lambda: { 'stake':0.0, 'pl':0.0, 'n':0 })
          for league, line, tier, stake, pl, dt in bets:
              # Determine tier preference: row tier -> fallback by league -> bucket 5
              if tier is None or (isinstance(tier, float) and math.isnan(tier)):
                  tier_val = LEAGUE_TIERS.get(league, 5)
              else:
                  try:
                      tier_val = int(tier)
                  except Exception:
                      tier_val = LEAGUE_TIERS.get(league, 5)
              line_binned = normalize_line(line)
              key = (tier_val, line_binned)
              heatmap[key]['stake'] += float(stake or 0)
              heatmap[key]['pl'] += float(pl or 0)
              heatmap[key]['n'] += 1

          # Write roi_heatmap.csv
          with open('site/roi_heatmap.csv','w', newline='', encoding='utf-8') as f:
              w = csv.writer(f); w.writerow(["tier","line","roi_pct","n"]) 
              for (tier_val, line_binned), agg in sorted(heatmap.items(), key=lambda kv: (kv[0][0], kv[0][1])):
                  roi_pct = (agg['pl']/agg['stake']*100.0) if agg['stake']>0 else 0.0
                  w.writerow([tier_val, line_binned, round(roi_pct, 2), agg['n']])

          # Write top_segments.csv (min n=30)
          items = []
          for (tier_val, line_binned), agg in heatmap.items():
              if agg['stake']>0 and agg['n']>=30:
                  roi_pct = agg['pl']/agg['stake']*100.0
                  items.append((tier_val, line_binned, roi_pct, agg['n']))
          items.sort(key=lambda x: x[2], reverse=True)
          with open('site/top_segments.csv','w', newline='', encoding='utf-8') as f:
              w = csv.writer(f); w.writerow(["tier","line","roi_pct","n"]) 
              for row in items[:50]:
                  w.writerow([row[0], row[1], round(row[2],2), row[3]])
          print("Built roi_heatmap.csv and top_segments.csv")
          PY

      - name: Build artifacts index
        shell: bash
        run: |
          cat > site/artifacts.html <<'HTML'
          <!doctype html>
          <html lang="en">
          <head>
            <meta charset="utf-8"/>
            <meta name="viewport" content="width=device-width, initial-scale=1"/>
            <title>Football Model Artifacts</title>
            <style>
              body { background:#1D262D; color:#E8E4D9; font-family: system-ui, sans-serif; padding:24px; }
              a { color:#2CB191; }
              h1 { color:#DCE667; margin-bottom: 8px; }
              .card { background:#202a31; padding:16px; border-radius:10px; margin:12px 0; }
              .grid { display:grid; gap:12px; grid-template-columns: repeat(auto-fit,minmax(240px,1fr)); }
            </style>
            </head>
            <body>
              <h1>Football Model – Latest Artifacts</h1>
              <div class="grid">
                <div class="card"><h3>Report</h3>
                  <p><a href="report.html">Open report.html</a></p>
                </div>
                <div class="card"><h3>Metrics</h3>
                  <p><a href="metrics.csv">metrics.csv</a></p>
                </div>
                <div class="card"><h3>Recommendations</h3>
                  <p><a href="recommendations.csv">recommendations.csv</a></p>
                  <p><a href="latest_recommendations.csv">latest_recommendations.csv</a></p>
                </div>
                <div class="card"><h3>Datasets</h3>
                  <p><a href="pnl_by_month.csv">pnl_by_month.csv</a></p>
                  <p><a href="bankroll_series_90d.csv">bankroll_series_90d.csv</a></p>
                </div>
              </div>
              <p style="opacity:.7;margin-top:24px;">Updated: ${GITHUB_RUN_NUMBER}</p>
            </body>
          </html>
          HTML

      - name: Configure Pages
        uses: actions/configure-pages@v5
        continue-on-error: true

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: site
        continue-on-error: true

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
        continue-on-error: true

