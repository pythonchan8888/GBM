name: daily-predict

on:
  schedule:
    - cron: "0 6 * * *"  # daily at 06:00 UTC
  workflow_dispatch:

permissions:
  contents: write  # Changed to write for commit/push
  pages: write
  id-token: write

jobs:
  build-and-publish:
    runs-on: ubuntu-latest
    env:
      DATABASE_URL: ${{ secrets.DATABASE_URL }}
      GBM_QUICK: ${{ vars.GBM_QUICK || '0' }}
      PARLAY_RETENTION_DAYS: ${{ vars.PARLAY_RETENTION_DAYS || '180' }}
      XAI_API_KEY: ${{ secrets.XAI_API_KEY }}
      FOOTYSTATS_API_KEY: ${{ secrets.FOOTYSTATS_API_KEY }}  # If needed for daily runs

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: true  # For commit/push later

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        shell: bash
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install numpy pandas requests lightgbm scikit-learn scipy seaborn matplotlib psycopg2-binary shap
          fi
          pip install gspread oauth2client  # Added for Google Sheets support

      - name: Apply database safeguards
        if: ${{ env.DATABASE_URL != '' }}
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          python - <<'PY'
          import os, psycopg2
          db_url = os.environ.get('DATABASE_URL')
          if db_url:
              try:
                  conn = psycopg2.connect(db_url)
                  cur = conn.cursor()
                  
                  # First ensure tables exist with proper constraints
                  print("Creating/ensuring tables exist...")
                  cur.execute("""
                  CREATE TABLE IF NOT EXISTS runs(
                      run_id text PRIMARY KEY,
                      started_at timestamptz,
                      finished_at timestamptz,
                      train_rows int,
                      test_rows int,
                      poisson_home_loss numeric,
                      poisson_away_loss numeric
                  );
                  """)
                  
                  cur.execute("""
                  CREATE TABLE IF NOT EXISTS recommendations(
                      run_id text,
                      dt_gmt8 timestamptz,
                      league text,
                      home text,
                      away text,
                      rec_text text,
                      line numeric,
                      odds numeric,
                      ev numeric,
                      confidence text,
                      kings_call_insight text,
                      kings_call_agreement text,
                      kings_call_reasoning text,
                      kings_call_sources text
                  );
                  """)
                  
                  # Check if unique constraint exists
                  cur.execute("""
                  SELECT COUNT(*) FROM pg_constraint 
                  WHERE conname = 'unique_recommendation_per_match_per_run' 
                  AND conrelid = 'recommendations'::regclass
                  """)
                  constraint_exists = cur.fetchone()[0] > 0
                  
                  if not constraint_exists:
                      cur.execute("""
                      ALTER TABLE recommendations 
                      ADD CONSTRAINT unique_recommendation_per_match_per_run 
                      UNIQUE (run_id, dt_gmt8, home, away, line)
                      """)
                      print("✅ Added unique constraint for recommendations")
                  else:
                      print("ℹ️  Unique constraint already exists")
                  
                  # Add King's Call columns if they don't exist (migration)
                  try:
                      cur.execute("ALTER TABLE recommendations ADD COLUMN IF NOT EXISTS kings_call_insight text")
                      cur.execute("ALTER TABLE recommendations ADD COLUMN IF NOT EXISTS kings_call_agreement text")
                      cur.execute("ALTER TABLE recommendations ADD COLUMN IF NOT EXISTS kings_call_reasoning text")
                      cur.execute("ALTER TABLE recommendations ADD COLUMN IF NOT EXISTS kings_call_sources text")
                      print("✅ King's Call columns added/verified")
                  except Exception as e:
                      print(f"⚠️  King's Call column migration: {e}")
                  
                  # Add performance indexes (safe to run multiple times)
                  cur.execute("""
                  CREATE INDEX IF NOT EXISTS idx_recommendations_datetime_ev 
                  ON recommendations (dt_gmt8 DESC, ev DESC)
                  """)
                  # Use a simpler index without the problematic WHERE clause
                  cur.execute("""
                  CREATE INDEX IF NOT EXISTS idx_recommendations_recent 
                  ON recommendations (dt_gmt8 DESC)
                  """)
                  
                  conn.commit()
                  cur.close()
                  conn.close()
                  print("✅ Database safeguards applied successfully")
              except Exception as e:
                  print(f"❌ Database safeguards failed: {e}")
                  raise  # Fail the workflow if safeguards fail
          PY

      - name: Run daily prediction pipeline
        env:
          FOOTYSTATS_API_KEY: ${{ secrets.FOOTYSTATS_API_KEY }}
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          python gbm_dc_ev_model.py

      - name: Retry prediction if failed
        if: failure()
        env:
          FOOTYSTATS_API_KEY: ${{ secrets.FOOTYSTATS_API_KEY }}
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          echo "⚠️ Retrying prediction pipeline..."
          python gbm_dc_ev_model.py || echo "Retry also failed, continuing with deployment"

      - name: Settle open bets (update P&L)
        if: ${{ env.DATABASE_URL != '' }}
        env:
          FOOTYSTATS_API_KEY: ${{ secrets.FOOTYSTATS_API_KEY }}
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          python - <<'PY'
          import os
          from gbm_dc_ev_model import settle_open_bets
          api_key = os.environ.get('FOOTYSTATS_API_KEY', '')
          db = os.environ.get('DATABASE_URL', '')
          settle_open_bets(api_key, db)
          PY

      - name: Prepare site folder
        run: |
          mkdir -p site
          # Copy ML model artifacts
          if [ -d artifacts/latest ]; then
            cp -r artifacts/latest/* site/ || true
          fi
          # Use committed assets under site/ as the single source of truth
          # Avoid copying root-level files over site/ to prevent overwriting newer site assets
          [ -f site/index.html ] || echo "❌ site/index.html missing"
          [ -f site/analytics.html ] || echo "❌ site/analytics.html missing"
          [ -f site/recommendations.html ] || echo "❌ site/recommendations.html missing"
          [ -f site/styles.css ] || echo "❌ site/styles.css missing"
          [ -f site/app.js ] || echo "❌ site/app.js missing"
          # Copy static assets
          [ -f favicon.svg ] && cp favicon.svg site/ || true
          [ -f site/favicon.svg ] && cp site/favicon.svg site/ || true
          [ -f site/preview-image.svg ] && cp site/preview-image.svg site/ || true
          # Ensure hero background images are in place (they should already be in site/)
          [ -f site/parlayking-social-preview.png ] && echo "✅ Social preview found" || echo "❌ Social preview missing"
          [ -f site/hero-stadium-background.png ] && echo "✅ Hero background found" || echo "❌ Hero background missing"
          # Verify dashboard files were copied successfully
          if [ -f site/index.html ] && [ -f site/styles.css ] && [ -f site/app.js ]; then
            echo "✅ ParlayKing dashboard files copied successfully"
          else
            echo "❌ Warning: Some dashboard files missing"
          fi
          echo "Listing site/ contents:"
          ls -la site/ || true

      - name: Enhanced Cache Busting
        run: |
          COMMIT_HASH=$(git rev-parse --short HEAD)
          echo "COMMIT_HASH: $COMMIT_HASH"
          
          # Use sed to replace the asset links in all HTML files (idempotent)
          # Remove any existing version params first, then add new ones
          for html_file in site/*.html; do
            if [ -f "$html_file" ]; then
              # Remove existing version parameters first
              sed -i "s|href=\"styles\.css[^\"]*\"|href=\"styles.css\"|g" "$html_file"
              sed -i "s|src=\"app\.js[^\"]*\"|src=\"app.js\"|g" "$html_file"
              # Add new version parameters
              sed -i "s|href=\"styles\.css\"|href=\"styles.css?v=${COMMIT_HASH}\"|g" "$html_file"
              sed -i "s|src=\"app\.js\"|src=\"app.js?v=${COMMIT_HASH}\"|g" "$html_file"
              echo "✅ Cache busted $html_file"
            fi
          done
          
          # Enhanced: Bust cache for images in CSS files
          if [ -f "site/styles.css" ]; then
            sed -i "s|url('hero-stadium-background\.png')|url('hero-stadium-background.png?v=${COMMIT_HASH}')|g" site/styles.css
            sed -i "s|url('parlayking-social-preview\.png')|url('parlayking-social-preview.png?v=${COMMIT_HASH}')|g" site/styles.css
            sed -i "s|url('favicon\.svg')|url('favicon.svg?v=${COMMIT_HASH}')|g" site/styles.css
            echo "✅ Cache busted images in CSS"
          fi

      - name: Minify Assets
        continue-on-error: true
        run: |
          echo "Installing minification tools..."
          npm install -g uglify-js clean-css-cli || echo "Warning: npm install failed, continuing..."
          
          echo "Minifying JavaScript..."
          if command -v uglifyjs &> /dev/null; then
            uglifyjs site/app.js -c -m -o site/app.min.js
            sed -i 's/app.js/app.min.js/g' site/*.html
            echo "✅ JavaScript minified successfully"
          else
            echo "⚠️  uglifyjs not available, copying original file"
            cp site/app.js site/app.min.js
          fi
          
          echo "Minifying CSS..."
          if command -v cleancss &> /dev/null; then
            cleancss -o site/styles.min.css site/styles.css
            sed -i 's/styles.css/styles.min.css/g' site/*.html
            echo "✅ CSS minified successfully"
          else
            echo "⚠️  cleancss not available, copying original file"
            cp site/styles.css site/styles.min.css
          fi

      - name: Optimize Images
        continue-on-error: true
        run: |
          echo "Installing image optimization tools..."
          npm install -g imagemin-cli imagemin-webp || echo "Warning: imagemin install failed, continuing..."
          
          echo "Converting images to WebP..."
          if [ -f "site/hero-stadium-background.png" ] && command -v imagemin &> /dev/null; then
            imagemin site/hero-stadium-background.png --plugin=webp > site/hero-stadium-background.webp
            if [ -f "site/hero-stadium-background.webp" ]; then
              sed -i 's/hero-stadium-background\.png/hero-stadium-background.webp/g' site/*.html site/styles.css
              echo "✅ Images optimized successfully"
            else
              echo "⚠️  WebP conversion failed, keeping original"
            fi
          else
            echo "⚠️  Image optimization tools not available or PNG not found, skipping"
          fi

      - name: Export metrics and datasets to CSV (from Postgres)
        if: ${{ env.DATABASE_URL != '' }}
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        shell: bash
        run: |
          python - <<'PY'
          import os, csv
          import psycopg2
          from datetime import datetime
          
          db = os.environ.get('DATABASE_URL')
          retention_days = int(os.environ.get('PARLAY_RETENTION_DAYS','180'))
          if not db:
              raise SystemExit(0)
          
          try:
              conn = psycopg2.connect(db); cur = conn.cursor()
              # Latest run
              cur.execute("""
              select run_id, started_at, finished_at, train_rows, test_rows, poisson_home_loss, poisson_away_loss
              from runs order by finished_at desc nulls last limit 1
              """)
              latest = cur.fetchone()
              metrics_rows = []
              if latest:
                  keys = ["run_id","started_at","finished_at","train_rows","test_rows","poisson_home_loss","poisson_away_loss"]
                  for k, v in zip(keys, latest):
                      metrics_rows.append((k, v if v is not None else ""))
              # 30-day aggregates
              cur.execute("""
              select coalesce(sum(pl),0) as pnl_30d, coalesce(sum(stake),0) as stake_30d, count(*) as bets_30d
              from bets
              where dt_gmt8 > now() - interval '30 days'
              """)
              pnl_30d, stake_30d, bets_30d = cur.fetchone()
              roi_30d = (float(pnl_30d)/float(stake_30d)*100.0) if stake_30d and float(stake_30d)>0 else 0.0
              
              # Win rate and performance metrics (30-day)
              cur.execute("""
              select 
                count(case when pl > 0 then 1 end) as wins,
                count(case when pl >= 0 then 1 end) as non_losing,
                count(*) as total_bets,
                avg(case when pl > 0 then (pl/stake)*100.0 end) as avg_win_return_pct
              from bets 
              where dt_gmt8 > now() - interval '30 days' and stake > 0
              """)
              win_stats = cur.fetchone()
              if win_stats and win_stats[2] > 0:
                  win_rate = (float(win_stats[0]) / float(win_stats[2])) * 100.0
                  non_losing_rate = (float(win_stats[1]) / float(win_stats[2])) * 100.0
                  avg_win_return = float(win_stats[3] or 0)
              else:
                  win_rate = non_losing_rate = avg_win_return = 0.0
              
              metrics_rows += [
                  ("pnl_30d", pnl_30d), ("stake_30d", stake_30d), ("bets_30d", bets_30d), ("roi_30d_pct", roi_30d),
                  ("win_rate_30d_pct", win_rate), ("non_losing_rate_30d_pct", non_losing_rate), 
                  ("avg_win_return_30d_pct", avg_win_return)
              ]
              # Write metrics.csv
              os.makedirs('site', exist_ok=True)
              with open('site/metrics.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["metric","value"]); w.writerows(metrics_rows)
              # P&L by month & league
              cur.execute("""
              select to_char(date_trunc('month', dt_gmt8),'YYYY-MM') as month, league, sum(pl) as pnl
              from bets group by 1,2 order by 1,2
              """)
              rows = cur.fetchall()
              with open('site/pnl_by_month.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["month","league","pnl"]); w.writerows(rows)
              # Bankroll series (last 90 days) with clean formatting
              cur.execute("""
              select dt_gmt8, ROUND(CAST(cum_bankroll AS NUMERIC), 2) as cum_bankroll 
              from bets
              where dt_gmt8 > now() - interval '90 days' order by dt_gmt8
              """)
              rows = cur.fetchall()
              with open('site/bankroll_series_90d.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["dt_gmt8","cum_bankroll"]); w.writerows(rows)
              # Latest recommendations (7 days) - deduplicated by best EV per match with clean formatting
              cur.execute("""
              SELECT dt_gmt8, league, home, away, rec_text, 
                     ROUND(CAST(line AS NUMERIC), 2) as line,
                     ROUND(CAST(odds AS NUMERIC), 2) as odds, 
                     ROUND(CAST(ev AS NUMERIC), 4) as ev, 
                     confidence,
                     kings_call_insight,
                     kings_call_agreement,
                     kings_call_reasoning,
                     kings_call_sources
              FROM (
                  SELECT dt_gmt8, league, home, away, rec_text, line, odds, ev, confidence,
                         kings_call_insight, kings_call_agreement, kings_call_reasoning, kings_call_sources,
                         ROW_NUMBER() OVER (PARTITION BY dt_gmt8, home, away ORDER BY ev DESC) as rn
                  FROM recommendations 
                  WHERE dt_gmt8 >= now() - interval '7 days'
              ) ranked
              WHERE rn = 1
              ORDER BY dt_gmt8, ev DESC
              """)
              rows = cur.fetchall()
              with open('site/latest_recommendations.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["dt_gmt8","league","home","away","rec_text","line","odds","ev","confidence","kings_call_insight","kings_call_agreement","kings_call_reasoning","kings_call_sources"]); w.writerows(rows)
              # Settled bets for parlay calculation (retention days)
              cur.execute(f"""
              select fixture_id, league, home, away, home_score, away_score,
                     line_betted_on_refined, bet_type_refined_ah, odds_betted_on_refined, 
                     stake, pl, status, dt_gmt8
              from bets where status = 'settled' and dt_gmt8 >= now() - interval '{retention_days} days' order by dt_gmt8 desc
              """)
              rows = cur.fetchall()
              with open('site/settled_bets.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["fixture_id","league","home","away","home_score","away_score","line_betted_on_refined","bet_type_refined_ah","odds_betted_on_refined","stake","pl","status","dt_gmt8"]); w.writerows(rows)
              cur.close(); conn.close()
              print("Exported metrics and datasets to site/.")
              
              # --- Build ROI heatmap (Tier x AH line) and Top Segments from bets ---
              import math
              import collections
              
              # Reconnect for a new cursor
              conn = psycopg2.connect(db); cur = conn.cursor()
              # Try to get bets with needed fields; tolerate missing columns
              cur.execute("""
              SELECT 
                COALESCE(league, '') as league,
                COALESCE(line_betted_on_refined, line, 0) as line,
                COALESCE(tier, 1) as tier,
                COALESCE(stake, 0) as stake,
                COALESCE(pl, 0) as pl,
                dt_gmt8
              FROM bets
              WHERE dt_gmt8 > now() - interval '730 days'
              """)
              bets = cur.fetchall()
              cur.close(); conn.close()
          except Exception as e:
              print(f"Database connection failed: {e}")
              print("Creating fallback CSV files...")
              import math
              import collections
              os.makedirs('site', exist_ok=True)
              # Create empty fallback CSVs
              with open('site/metrics.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["metric","value"])
                  w.writerow(["status", "DB_unavailable"])
                  w.writerow(["generated_at", datetime.now().isoformat()])
              with open('site/pnl_by_month.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["month","league","pnl"])
              with open('site/bankroll_series_90d.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["dt_gmt8","cum_bankroll"])
              with open('site/latest_recommendations.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["dt_gmt8","league","home","away","rec_text","line","odds","ev","confidence","kings_call_insight","kings_call_agreement","kings_call_reasoning","kings_call_sources"])
              with open('site/settled_bets.csv','w', newline='', encoding='utf-8') as f:
                  w = csv.writer(f); w.writerow(["fixture_id","league","home","away","home_score","away_score","line_betted_on_refined","bet_type_refined_ah","odds_betted_on_refined","stake","pl","status","dt_gmt8"])
              bets = []  # Empty list for heatmap processing

          # Static fallback for tier mapping if DB rows don't contain a tier
          LEAGUE_TIERS = {
              'England Premier League': 1, 'Spain La Liga': 1, 'Germany Bundesliga': 1, 'Italy Serie A': 1, 'France Ligue 1': 1,
              'England Championship': 2, 'Netherlands Eredivisie': 2, 'Portugal Liga NOS': 2, 'Belgium Pro League': 2, 'Scotland Premiership': 2,
              'Turkey Süper Lig': 3, 'Austria Bundesliga': 3, 'Denmark Superliga': 3, 'USA MLS': 3, 'Brazil Serie A': 3,
              'Argentina Primera División': 3, 'Japan J1 League': 3, 'South Korea K League 1': 3, 'Saudi Arabia Professional League': 3
          }

          def normalize_line(x):
              # Bin to nearest 0.25 like -1, -0.75, ...
              try:
                  return round(float(x) * 4) / 4.0
              except Exception:
                  return 0.0

          heatmap = collections.defaultdict(lambda: { 'stake':0.0, 'pl':0.0, 'n':0 })
          for league, line, tier, stake, pl, dt in bets:
              # Determine tier preference: row tier -> fallback by league -> bucket 5
              if tier is None or (isinstance(tier, float) and math.isnan(tier)):
                  tier_val = LEAGUE_TIERS.get(league, 5)
              else:
                  try:
                      tier_val = int(tier)
                  except Exception:
                      tier_val = LEAGUE_TIERS.get(league, 5)
              line_binned = normalize_line(line)
              key = (tier_val, line_binned)
              heatmap[key]['stake'] += float(stake or 0)
              heatmap[key]['pl'] += float(pl or 0)
              heatmap[key]['n'] += 1

          # Write roi_heatmap.csv
          with open('site/roi_heatmap.csv','w', newline='', encoding='utf-8') as f:
              w = csv.writer(f); w.writerow(["tier","line","roi_pct","n"]) 
              for (tier_val, line_binned), agg in sorted(heatmap.items(), key=lambda kv: (kv[0][0], kv[0][1])):
                  roi_pct = (agg['pl']/agg['stake']*100.0) if agg['stake']>0 else 0.0
                  w.writerow([tier_val, line_binned, round(roi_pct, 2), agg['n']])

          # Write top_segments.csv (min n=30)
          items = []
          for (tier_val, line_binned), agg in heatmap.items():
              if agg['stake']>0 and agg['n']>=30:
                  roi_pct = agg['pl']/agg['stake']*100.0
                  items.append((tier_val, line_binned, roi_pct, agg['n']))
          items.sort(key=lambda x: x[2], reverse=True)
          with open('site/top_segments.csv','w', newline='', encoding='utf-8') as f:
              w = csv.writer(f); w.writerow(["tier","line","roi_pct","n"]) 
              for row in items[:50]:
                  w.writerow([row[0], row[1], round(row[2],2), row[3]])
          print("Built roi_heatmap.csv and top_segments.csv")
          PY

      - name: Build parlay wins CSV
        run: |
          python - <<'PY'
          import csv
          from datetime import datetime, timedelta, timezone
          from collections import defaultdict
          import itertools, os
          os.makedirs('site', exist_ok=True)
          retention_days = int(os.environ.get('PARLAY_RETENTION_DAYS','180'))
          bets = []
          try:
              with open('site/settled_bets.csv', newline='', encoding='utf-8') as f:
                  reader = csv.DictReader(f)
                  for row in reader:
                      dt = row.get('dt_gmt8') or ''
                      try:
                          dt_obj = datetime.fromisoformat(dt.replace(' ', 'T'))
                      except Exception:
                          try:
                              dt_obj = datetime.strptime(dt, '%Y-%m-%d %H:%M:%S')
                          except Exception:
                              continue
                      # restrict to retention window
                      try:
                          if dt_obj < datetime.now() - timedelta(days=retention_days):
                              continue
                      except Exception:
                          pass
                      pl = float(row.get('pl') or 0)
                      if pl < 0:
                          continue
                      odds = float(row.get('odds_betted_on_refined') or row.get('odds') or 1.0)
                      if abs(pl) < 1e-9:
                          odds = 1.0  # push
                      home = row.get('home','')
                      away = row.get('away','')
                      line = row.get('line_betted_on_refined') or row.get('line') or '0'
                      try:
                          line = float(line)
                      except Exception:
                          line = 0.0
                      bet_type = (row.get('bet_type_refined_ah') or row.get('bet_type') or '').lower()
                      if 'away' in bet_type:
                          rec = f"{away} {'+' if line>=0 else ''}{line:.2f}"
                      elif 'home' in bet_type:
                          rec = f"{home} {'+' if line>=0 else ''}{line:.2f}"
                      else:
                          rec = f"AH {'+' if line>=0 else ''}{line:.2f}"
                      bets.append({'dt': dt_obj, 'home': home, 'away': away, 'odds': max(1.0, odds), 'rec': rec})
          except FileNotFoundError:
              pass

          # Group by 12-hour windows anchored at 05:00 and 17:00 GMT+8
          buckets = defaultdict(list)
          def to_gmt8(dt):
              return dt + timedelta(hours=8)
          def from_gmt8(y, m, d, hh):
              # return naive datetime in local (same representation as inputs)
              return datetime(y, m, d, hh) - timedelta(hours=8)
          def anchor_for(dt):
              g8 = to_gmt8(dt)
              y, m, d, h = g8.year, g8.month, g8.day, g8.hour
              if 5 <= h < 17:
                  return from_gmt8(y, m, d, 5)
              elif h >= 17:
                  return from_gmt8(y, m, d, 17)
              else:
                  prev = g8 - timedelta(days=1)
                  return from_gmt8(prev.year, prev.month, prev.day, 17)
          for b in bets:
              key = anchor_for(b['dt'])
              buckets[key].append(b)

          def legs_str(legs):
              return ' || '.join([f"{l['home']} vs {l['away']} | {l['rec']}@{float(l['odds']):.2f}" for l in legs])

          results = []
          for key, legs in buckets.items():
              legs = sorted(legs, key=lambda x: x['dt'])[:10]
              for size in range(3, min(6, len(legs)) + 1):
                  scored = []
                  for combo in itertools.combinations(legs, size):
                      prod = 1.0
                      for lg in combo:
                          prod *= float(lg['odds'] or 1.0)
                      scored.append((prod, combo))
                  scored.sort(key=lambda x: x[0], reverse=True)
                  for prod, combo in scored[:2]:
                      stake = 100.0
                      payout = stake * prod
                      profit = payout - stake
                      start = min(l['dt'] for l in combo)
                      end = max(l['dt'] for l in combo)
                      results.append([
                          start.isoformat(sep=' '), end.isoformat(sep=' '), len(combo),
                          round(prod, 4), stake, round(payout, 2), round(profit, 2),
                          legs_str(combo)
                      ])

          with open('site/parlay_wins.csv','w', newline='', encoding='utf-8') as f:
              w = csv.writer(f)
              w.writerow(['window_start','window_end','leg_count','total_odds','stake','payout','profit','legs'])
              for row in sorted(results, key=lambda r: r[6], reverse=True)[:20]:
                  w.writerow(row)
          print('Built parlay_wins.csv')
          PY

      - name: Validate King's Call (non-blocking)
        shell: bash
        run: |
          echo "🔎 Validating King's Call tweets in latest_recommendations.csv (non-blocking)"
          python - <<'PY'
          import csv, os
          path = 'site/latest_recommendations.csv'
          if not os.path.isfile(path):
              print('ℹ️  latest_recommendations.csv not found; skipping validation')
              raise SystemExit(0)
          with open(path, newline='', encoding='utf-8') as f:
              r = csv.DictReader(f)
              rows = list(r)
          total = len(rows)
          placeholder = "King's insight: Solid bet based on ML analysis—watch this space!"
          missing = sum(1 for row in rows if not (row.get('kings_call') or '').strip() or (row.get('kings_call') or '') == placeholder)
          if total == 0:
              print('ℹ️  No recommendation rows to validate.')
          else:
              pct = (missing / total) * 100.0
              print(f"King's Call missing: {missing}/{total} ({pct:.1f}%)")
              if missing == total:
                  print("⚠️  All King's Call entries are missing. Check XAI_API_KEY or API limits.")
              elif missing > 0:
                  print("ℹ️  Some King's Call entries are missing. This is informational only.")
          PY

      - name: Commit and push generated files
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git add site/* artifacts/latest/* || true
          git commit -m "Update recommendations and artifacts from daily run" || echo "No changes to commit"
          git push origin main
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Configure Pages
        uses: actions/configure-pages@v5
        continue-on-error: true

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: site
        continue-on-error: true

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
        continue-on-error: true

      - name: Notify on failure
        if: failure()
        run: |
          echo "🚨 Daily workflow failed! Check the logs above for details."